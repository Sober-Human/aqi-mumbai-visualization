{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0451fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DIR: D:\\Downloads\\FieldProjectAQI_Data\\data\n",
      "IMAGES_DIR: D:\\Downloads\\FieldProjectAQI_Data\\images\n",
      "AQI melted: AQI_daily_city_level_mumbai_2023_mumbai_2023.xlsx rows: 365\n",
      "Prominent melted: AQI_daily_city_level_mumbai_2023_mumbai_2023.xlsx rows: 365\n",
      "AQI melted: AQI_daily_city_level_mumbai_2024_mumbai_2024.xlsx rows: 366\n",
      "Prominent melted: AQI_daily_city_level_mumbai_2024_mumbai_2024.xlsx rows: 366\n",
      "AQI melted: AQI_daily_city_level_mumbai_2025_mumbai_2025.xlsx rows: 90\n",
      "Prominent melted: AQI_daily_city_level_mumbai_2025_mumbai_2025.xlsx rows: 90\n",
      "Prominent melted: Prominent_param2023.xlsx rows: 365\n",
      "Prominent melted: Prominent_param2025.xlsx rows: 90\n",
      "Prominent melted: prominent_param_2024.xlsx rows: 366\n",
      "Combined AQI rows: 821\n",
      "Combined Prominent pollutant rows: 1642\n",
      "Saved cleaned CSV: D:\\Downloads\\FieldProjectAQI_Data\\data\\cleaned_aqi.csv\n",
      "Saved prominent CSV: D:\\Downloads\\FieldProjectAQI_Data\\data\\prominent_pollutant.csv\n"
     ]
    }
   ],
   "source": [
    "# CELL 1: create cleaned_aqi.csv and prominent_pollutant.csv\n",
    "import os, glob, pandas as pd, calendar, re, numpy as np\n",
    "DATA_ROOT = r\"D:\\Downloads\\FieldProjectAQI_Data\"\n",
    "DATA_DIR = os.path.join(DATA_ROOT, \"data\")\n",
    "IMAGES_DIR = os.path.join(DATA_ROOT, \"images\")\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(IMAGES_DIR, exist_ok=True)\n",
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "print(\"IMAGES_DIR:\", IMAGES_DIR)\n",
    "\n",
    "# helper to melt calendar layout (Day x Month -> rows)\n",
    "def melt_calendar_aqi(path, sheet_name='AQI', year_hint=None):\n",
    "    xls = pd.ExcelFile(path)\n",
    "    if sheet_name not in xls.sheet_names:\n",
    "        # fallback: pick first sheet\n",
    "        sheet = xls.sheet_names[0]\n",
    "    else:\n",
    "        sheet = sheet_name\n",
    "    df = pd.read_excel(path, sheet_name=sheet)\n",
    "    df.columns = [str(c).strip() for c in df.columns]\n",
    "    # find Day column\n",
    "    day_col = next((c for c in df.columns if str(c).strip().lower().startswith('day')), None)\n",
    "    if day_col is None:\n",
    "        raise ValueError(f\"No Day column in {path} sheet {sheet}\")\n",
    "    months = [c for c in df.columns if c != day_col]\n",
    "    long = df.melt(id_vars=[day_col], value_vars=months, var_name='MonthName', value_name='AQI')\n",
    "    long = long.dropna(subset=['AQI']).reset_index(drop=True)\n",
    "    long[day_col] = pd.to_numeric(long[day_col], errors='coerce').astype('Int64')\n",
    "    month_map = {calendar.month_name[i]: i for i in range(1,13)}\n",
    "    month_map.update({calendar.month_abbr[i]: i for i in range(1,13)})\n",
    "    long['Month'] = long['MonthName'].map(lambda x: month_map.get(str(x).strip(), np.nan)).astype('Int64')\n",
    "    if year_hint is None:\n",
    "        m = re.search(r\"(20\\d{2})\", os.path.basename(path))\n",
    "        year_hint = int(m.group(1)) if m else None\n",
    "    if year_hint is None:\n",
    "        raise ValueError(\"Year not found for file: \" + path)\n",
    "    def mkdate(r):\n",
    "        try:\n",
    "            return pd.Timestamp(year=year_hint, month=int(r['Month']), day=int(r[day_col]))\n",
    "        except:\n",
    "            return pd.NaT\n",
    "    long['Date'] = long.apply(mkdate, axis=1)\n",
    "    long = long.dropna(subset=['Date']).reset_index(drop=True)\n",
    "    long['AQI'] = pd.to_numeric(long['AQI'], errors='coerce')\n",
    "    long['Year'] = year_hint\n",
    "    long = long.rename(columns={day_col:'Day'})\n",
    "    return long[['Date','Year','Month','Day','AQI']]\n",
    "\n",
    "# find files and melt only the AQI sheets\n",
    "xlsx_paths = glob.glob(os.path.join(DATA_DIR, \"*.xlsx\"))\n",
    "aqi_parts = []\n",
    "prom_parts = []\n",
    "for p in sorted(xlsx_paths):\n",
    "    xls = pd.ExcelFile(p)\n",
    "    # If file has 'AQI' sheet, melt AQI\n",
    "    if any(s.lower()=='aqi' for s in xls.sheet_names):\n",
    "        try:\n",
    "            part = melt_calendar_aqi(p, sheet_name='AQI')\n",
    "            print(\"AQI melted:\", os.path.basename(p), \"rows:\", len(part))\n",
    "            aqi_parts.append(part)\n",
    "        except Exception as e:\n",
    "            print(\"AQI melt failed for\", p, \":\", e)\n",
    "    # If file has a 'Prominent' or 'Prominent Parameters' sheet, melt pollutant\n",
    "    poll_sheet = next((s for s in xls.sheet_names if 'prominent' in s.lower() or 'prominent parameter' in s.lower()), None)\n",
    "    if poll_sheet:\n",
    "        try:\n",
    "            # melt pollutant sheet similarly but keep pollutant strings\n",
    "            dfp = pd.read_excel(p, sheet_name=poll_sheet)\n",
    "            dfp.columns = [str(c).strip() for c in dfp.columns]\n",
    "            day_col = next((c for c in dfp.columns if str(c).strip().lower().startswith('day')), None)\n",
    "            if day_col:\n",
    "                months = [c for c in dfp.columns if c != day_col]\n",
    "                longp = dfp.melt(id_vars=[day_col], value_vars=months, var_name='MonthName', value_name='Pollutant')\n",
    "                longp = longp.dropna(subset=['Pollutant']).reset_index(drop=True)\n",
    "                longp[day_col] = pd.to_numeric(longp[day_col], errors='coerce').astype('Int64')\n",
    "                month_map = {calendar.month_name[i]: i for i in range(1,13)}\n",
    "                month_map.update({calendar.month_abbr[i]: i for i in range(1,13)})\n",
    "                longp['Month'] = longp['MonthName'].map(lambda x: month_map.get(str(x).strip(), np.nan)).astype('Int64')\n",
    "                m = re.search(r\"(20\\d{2})\", os.path.basename(p))\n",
    "                year_hint = int(m.group(1)) if m else None\n",
    "                def mkdatep(r):\n",
    "                    try:\n",
    "                        return pd.Timestamp(year=year_hint, month=int(r['Month']), day=int(r[day_col]))\n",
    "                    except:\n",
    "                        return pd.NaT\n",
    "                longp['Date'] = longp.apply(mkdatep, axis=1)\n",
    "                longp = longp.dropna(subset=['Date']).reset_index(drop=True)\n",
    "                longp['Year'] = year_hint\n",
    "                longp = longp.rename(columns={day_col:'Day'})\n",
    "                longp = longp[['Date','Year','Month','Day','Pollutant']]\n",
    "                print(\"Prominent melted:\", os.path.basename(p), \"rows:\", len(longp))\n",
    "                prom_parts.append(longp)\n",
    "        except Exception as e:\n",
    "            print(\"Prominent melt failed for\", p, \":\", e)\n",
    "\n",
    "if not aqi_parts:\n",
    "    raise SystemExit(\"No AQI parts found â€” check file locations.\")\n",
    "all_long = pd.concat(aqi_parts, ignore_index=True)\n",
    "print(\"Combined AQI rows:\", len(all_long))\n",
    "\n",
    "prom_all = pd.concat(prom_parts, ignore_index=True) if prom_parts else pd.DataFrame(columns=['Date','Year','Month','Day','Pollutant'])\n",
    "print(\"Combined Prominent pollutant rows:\", len(prom_all))\n",
    "\n",
    "# Save CSVs\n",
    "cleaned_path = os.path.join(DATA_DIR, \"cleaned_aqi.csv\")\n",
    "prom_path = os.path.join(DATA_DIR, \"prominent_pollutant.csv\")\n",
    "all_long.to_csv(cleaned_path, index=False)\n",
    "prom_all.to_csv(prom_path, index=False)\n",
    "print(\"Saved cleaned CSV:\", cleaned_path)\n",
    "print(\"Saved prominent CSV:\", prom_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da6b9927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting from rows: 821\n",
      "Saved: D:\\Downloads\\FieldProjectAQI_Data\\images\\3yr_monthly_avg_aqi.png\n",
      "Saved: D:\\Downloads\\FieldProjectAQI_Data\\images\\annual_avg_aqi.png\n",
      "Saved: D:\\Downloads\\FieldProjectAQI_Data\\images\\daily_trend_2023.png\n",
      "Saved: D:\\Downloads\\FieldProjectAQI_Data\\images\\daily_trend_2024.png\n",
      "Saved: D:\\Downloads\\FieldProjectAQI_Data\\images\\daily_trend_2025.png\n",
      "Saved: D:\\Downloads\\FieldProjectAQI_Data\\images\\aqi_heatmap_2023.png\n",
      "Saved: D:\\Downloads\\FieldProjectAQI_Data\\images\\aqi_heatmap_2024.png\n",
      "Saved: D:\\Downloads\\FieldProjectAQI_Data\\images\\aqi_heatmap_2025.png\n",
      "Saved: D:\\Downloads\\FieldProjectAQI_Data\\images\\aqi_category_distribution.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sober\\AppData\\Local\\Temp\\ipykernel_12340\\84807093.py:105: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  plt.boxplot(monthly_boxes, labels=[calendar.month_abbr[m] for m in range(1,13)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Downloads\\FieldProjectAQI_Data\\images\\monthly_aqi_boxplot.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CELL 2: create PNG charts in images/ (uses matplotlib)\n",
    "import os, pandas as pd, numpy as np, matplotlib.pyplot as plt, calendar\n",
    "DATA_ROOT = r\"D:\\Downloads\\FieldProjectAQI_Data\"\n",
    "DATA_DIR = os.path.join(DATA_ROOT, \"data\")\n",
    "IMAGES_DIR = os.path.join(DATA_ROOT, \"images\")\n",
    "os.makedirs(IMAGES_DIR, exist_ok=True)\n",
    "\n",
    "all_long = pd.read_csv(os.path.join(DATA_DIR, \"cleaned_aqi.csv\"), parse_dates=['Date'])\n",
    "all_long['Year'] = all_long['Year'].astype(int)\n",
    "all_long['Month'] = all_long['Month'].astype(int)\n",
    "print(\"Plotting from rows:\", len(all_long))\n",
    "\n",
    "# 1) 3-Year Comparative Monthly Averages\n",
    "monthly_avg = all_long.groupby(['Year','Month'])['AQI'].mean().reset_index()\n",
    "pivot_monthly = monthly_avg.pivot(index='Month', columns='Year', values='AQI').reindex(index=range(1,13))\n",
    "plt.figure(figsize=(12,6))\n",
    "pivot_monthly.plot(kind='bar', width=0.8)\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Average AQI')\n",
    "plt.title('3-Year Comparative Monthly Average AQI (Mumbai)')\n",
    "plt.xticks(ticks=range(12), labels=[calendar.month_abbr[i+1] for i in range(12)], rotation=0)\n",
    "plt.tight_layout()\n",
    "p1 = os.path.join(IMAGES_DIR, \"3yr_monthly_avg_aqi.png\")\n",
    "plt.savefig(p1, dpi=200)\n",
    "plt.close()\n",
    "print(\"Saved:\", p1)\n",
    "\n",
    "# 2) Overall Annual AQI Trend (line)\n",
    "annual_avg = all_long.groupby('Year')['AQI'].mean().reset_index()\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(annual_avg['Year'], annual_avg['AQI'], marker='o', linewidth=2)\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Average AQI')\n",
    "plt.title('Overall Annual Average AQI')\n",
    "plt.grid(True)\n",
    "# OPTIONAL: set y-limits dynamically (e.g., pad top)\n",
    "ymax = max(annual_avg['AQI']) * 1.5\n",
    "plt.ylim(0, ymax)\n",
    "p2 = os.path.join(IMAGES_DIR, \"annual_avg_aqi.png\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(p2, dpi=200)\n",
    "plt.close()\n",
    "print(\"Saved:\", p2)\n",
    "\n",
    "# 3) Daily AQI Trends for Each Year\n",
    "for y in sorted(all_long['Year'].unique()):\n",
    "    dfy = all_long[all_long['Year']==y].sort_values('Date')\n",
    "    plt.figure(figsize=(14,4))\n",
    "    plt.plot(dfy['Date'], dfy['AQI'], linewidth=1)\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('AQI')\n",
    "    plt.title(f'Daily AQI Trend - {y}')\n",
    "    plt.tight_layout()\n",
    "    out = os.path.join(IMAGES_DIR, f\"daily_trend_{y}.png\")\n",
    "    plt.savefig(out, dpi=150)\n",
    "    plt.close()\n",
    "    print(\"Saved:\", out)\n",
    "\n",
    "# 4) AQI Heatmap (Month vs Day) for each year\n",
    "import numpy as np\n",
    "for y in sorted(all_long['Year'].unique()):\n",
    "    dfy = all_long[all_long['Year']==y].copy()\n",
    "    heat = dfy.pivot_table(index='Month', columns='Day', values='AQI', aggfunc='mean').reindex(index=range(1,13))\n",
    "    plt.figure(figsize=(14,6))\n",
    "    plt.imshow(heat, aspect='auto', origin='lower')\n",
    "    plt.colorbar(label='AQI')\n",
    "    plt.yticks(ticks=np.arange(12), labels=[calendar.month_name[i+1] for i in range(12)])\n",
    "    plt.xticks(ticks=np.arange(31), labels=list(range(1,32)))\n",
    "    plt.xlabel('Day of Month')\n",
    "    plt.ylabel('Month')\n",
    "    plt.title(f'AQI Heatmap (Month vs Day) - {y}')\n",
    "    plt.tight_layout()\n",
    "    out = os.path.join(IMAGES_DIR, f\"aqi_heatmap_{y}.png\")\n",
    "    plt.savefig(out, dpi=150)\n",
    "    plt.close()\n",
    "    print(\"Saved:\", out)\n",
    "\n",
    "# 5) AQI Category Distribution\n",
    "def aqi_category(a):\n",
    "    a = float(a)\n",
    "    if a <= 50: return 'Good'\n",
    "    if a <= 100: return 'Satisfactory'\n",
    "    if a <= 200: return 'Moderate'\n",
    "    if a <= 300: return 'Poor'\n",
    "    if a <= 400: return 'Very Poor'\n",
    "    return 'Severe'\n",
    "\n",
    "all_long['Category'] = all_long['AQI'].apply(aqi_category)\n",
    "cat_counts = all_long.groupby(['Year','Category']).size().reset_index(name='days')\n",
    "pivot_cat = cat_counts.pivot(index='Category', columns='Year', values='days').reindex(index=['Good','Satisfactory','Moderate','Poor','Very Poor','Severe']).fillna(0)\n",
    "plt.figure(figsize=(8,5))\n",
    "pivot_cat.plot(kind='bar')\n",
    "plt.xlabel('AQI Category')\n",
    "plt.ylabel('Days')\n",
    "plt.title('AQI Category Distribution by Year')\n",
    "plt.tight_layout()\n",
    "out = os.path.join(IMAGES_DIR, \"aqi_category_distribution.png\")\n",
    "plt.savefig(out, dpi=150)\n",
    "plt.close()\n",
    "print(\"Saved:\", out)\n",
    "\n",
    "# 6) Monthly AQI Distribution Boxplot\n",
    "monthly_boxes = [all_long[all_long['Month']==m]['AQI'].dropna().values for m in range(1,13)]\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.boxplot(monthly_boxes, labels=[calendar.month_abbr[m] for m in range(1,13)])\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('AQI')\n",
    "plt.title('Monthly AQI Distribution (Boxplot) - All years combined')\n",
    "plt.tight_layout()\n",
    "out = os.path.join(IMAGES_DIR, \"monthly_aqi_boxplot.png\")\n",
    "plt.savefig(out, dpi=150)\n",
    "plt.close()\n",
    "print(\"Saved:\", out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19fcee27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipped images to: D:\\Downloads\\FieldProjectAQI_Data\\images_backup.zip\n"
     ]
    }
   ],
   "source": [
    "# CELL 3: zip images for quick download (optional)\n",
    "import zipfile, os\n",
    "IMAGES_DIR = r\"D:\\Downloads\\FieldProjectAQI_Data\\images\"\n",
    "zip_path = r\"D:\\Downloads\\FieldProjectAQI_Data\\images_backup.zip\"\n",
    "with zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zf:\n",
    "    for root, dirs, files in os.walk(IMAGES_DIR):\n",
    "        for f in files:\n",
    "            if f.lower().endswith(\".png\"):\n",
    "                zf.write(os.path.join(root, f), arcname=f)\n",
    "print(\"Zipped images to:\", zip_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b186e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching recursively in: D:\\Downloads\\FieldProjectAQI_Data\n",
      "Found 6 .xlsx files (paths):\n",
      " - D:\\Downloads\\FieldProjectAQI_Data\\data\\AQI_daily_city_level_mumbai_2023_mumbai_2023.xlsx\n",
      " - D:\\Downloads\\FieldProjectAQI_Data\\data\\AQI_daily_city_level_mumbai_2024_mumbai_2024.xlsx\n",
      " - D:\\Downloads\\FieldProjectAQI_Data\\data\\AQI_daily_city_level_mumbai_2025_mumbai_2025.xlsx\n",
      " - D:\\Downloads\\FieldProjectAQI_Data\\data\\Prominent_param2023.xlsx\n",
      " - D:\\Downloads\\FieldProjectAQI_Data\\data\\Prominent_param2025.xlsx\n",
      " - D:\\Downloads\\FieldProjectAQI_Data\\data\\prominent_param_2024.xlsx\n",
      "\n",
      "File: D:\\Downloads\\FieldProjectAQI_Data\\data\\AQI_daily_city_level_mumbai_2023_mumbai_2023.xlsx\n",
      "  Sheets: ['AQI', 'Prominent Parameters']\n",
      "  Previewing sheet: AQI\n",
      " Day  January  February  March  April  May  June  July  August  September  October  November  December\n",
      "   1      147       227    174    108   74    89    67      54         84       87       175       127\n",
      "   2      188       180    205     93   78    84    54      58         81       97       169       138\n",
      "   3      214       198    262     86   80    74    62      62         80      116       178       144\n",
      "   4      204       201    281     86   76    72    65      73         86      121       180       128\n",
      "   5      223       226    260     87   75    73    65      62         81      120       198       136\n",
      "   6      255       185    246     99   74    81    51      68         78      131       198       154\n",
      "   7      204       177    139    149   84    87    54      70         71      120       186       125\n",
      "   8      198       233    136    182   82    89    91      71         65      110       131       117\n",
      "\n",
      "File: D:\\Downloads\\FieldProjectAQI_Data\\data\\AQI_daily_city_level_mumbai_2024_mumbai_2024.xlsx\n",
      "  Sheets: ['AQI', 'Prominent Parameters']\n",
      "  Previewing sheet: AQI\n",
      " Day  January  February  March  April  May  June  July  August  September  October  November  December\n",
      "   1      132        93    118    106  102    62    60      41         39       66       169       167\n",
      "   2      133        63     71    160   92    50    54      32         45      101       208       174\n",
      "   3      117       122     55    128  117    68    61      31         43       58       157       129\n",
      "   4      129       158     72     89   89    82    59      36         46       54       159       139\n",
      "   5      123       154    109     91   79    69    51      37         36       54       150       154\n",
      "   6      128       127    119    131   68    59    56      35         40       61       158       148\n",
      "   7      124       115    108     95   65    62    74      34         37      116       137       126\n",
      "   8       97       131    124     74   77    57    47      38         34      115       146       125\n",
      "\n",
      "File: D:\\Downloads\\FieldProjectAQI_Data\\data\\AQI_daily_city_level_mumbai_2025_mumbai_2025.xlsx\n",
      "  Sheets: ['AQI', 'Prominent Parameters']\n",
      "  Previewing sheet: AQI\n",
      " Day  January  February  March  April  May  June  July  August  September  October  November  December\n",
      "   1      140        89    109    NaN  NaN   NaN   NaN     NaN        NaN      NaN       NaN       NaN\n",
      "   2      119       122    109    NaN  NaN   NaN   NaN     NaN        NaN      NaN       NaN       NaN\n",
      "   3      126       128    111    NaN  NaN   NaN   NaN     NaN        NaN      NaN       NaN       NaN\n",
      "   4      154       111    120    NaN  NaN   NaN   NaN     NaN        NaN      NaN       NaN       NaN\n",
      "   5      162       113    117    NaN  NaN   NaN   NaN     NaN        NaN      NaN       NaN       NaN\n",
      "   6      132       128    121    NaN  NaN   NaN   NaN     NaN        NaN      NaN       NaN       NaN\n",
      "   7      143       166    130    NaN  NaN   NaN   NaN     NaN        NaN      NaN       NaN       NaN\n",
      "   8      137       163    126    NaN  NaN   NaN   NaN     NaN        NaN      NaN       NaN       NaN\n",
      "\n",
      "MELTED: D:\\Downloads\\FieldProjectAQI_Data\\data\\AQI_daily_city_level_mumbai_2023_mumbai_2023.xlsx -> 365 rows\n",
      "\n",
      "MELTED: D:\\Downloads\\FieldProjectAQI_Data\\data\\AQI_daily_city_level_mumbai_2024_mumbai_2024.xlsx -> 366 rows\n",
      "\n",
      "MELTED: D:\\Downloads\\FieldProjectAQI_Data\\data\\AQI_daily_city_level_mumbai_2025_mumbai_2025.xlsx -> 90 rows\n",
      "\n",
      "MELTED: D:\\Downloads\\FieldProjectAQI_Data\\data\\Prominent_param2023.xlsx -> 365 rows\n",
      "\n",
      "MELTED: D:\\Downloads\\FieldProjectAQI_Data\\data\\Prominent_param2025.xlsx -> 90 rows\n",
      "\n",
      "MELTED: D:\\Downloads\\FieldProjectAQI_Data\\data\\prominent_param_2024.xlsx -> 366 rows\n",
      "\n",
      "Combined all_long rows: 1642\n",
      "      Date  Year  Month  Day   AQI\n",
      "2023-01-01  2023      1    1 147.0\n",
      "2023-01-02  2023      1    2 188.0\n",
      "2023-01-03  2023      1    3 214.0\n",
      "2023-01-04  2023      1    4 204.0\n",
      "2023-01-05  2023      1    5 223.0\n"
     ]
    }
   ],
   "source": [
    "# RUN THIS CELL: recursive search for .xlsx, preview sheets, and try melting them\n",
    "import os, glob, pandas as pd, calendar, re, numpy as np\n",
    "DATA_ROOT = r\"D:\\Downloads\\FieldProjectAQI_Data\"   # confirm this is your folder\n",
    "\n",
    "print(\"Searching recursively in:\", DATA_ROOT)\n",
    "xlsx_paths = glob.glob(os.path.join(DATA_ROOT, \"**\", \"*.xlsx\"), recursive=True)\n",
    "print(f\"Found {len(xlsx_paths)} .xlsx files (paths):\")\n",
    "for p in xlsx_paths:\n",
    "    print(\" -\", p)\n",
    "\n",
    "if not xlsx_paths:\n",
    "    print(\"\\nNo .xlsx files found under the DATA_ROOT. Please ensure your Excel files (the CPCB downloads) are inside this folder or a subfolder (e.g., 'raw' or 'data/raw').\")\n",
    "else:\n",
    "    # preview up to 3 files\n",
    "    preview_count = min(3, len(xlsx_paths))\n",
    "    previews = []\n",
    "    for p in xlsx_paths[:preview_count]:\n",
    "        try:\n",
    "            xls = pd.ExcelFile(p)\n",
    "            print(f\"\\nFile: {p}\")\n",
    "            print(\"  Sheets:\", xls.sheet_names)\n",
    "            # pick a likely sheet: prefer sheet name containing 'AQI' or first sheet\n",
    "            sheet = next((s for s in xls.sheet_names if 'aqi' in s.lower()), xls.sheet_names[0])\n",
    "            print(\"  Previewing sheet:\", sheet)\n",
    "            df = pd.read_excel(p, sheet_name=sheet, nrows=8)\n",
    "            print(df.head(8).to_string(index=False))\n",
    "            previews.append((p, sheet, df.columns.tolist()))\n",
    "        except Exception as e:\n",
    "            print(\"  Could not read file/sheet:\", e)\n",
    "\n",
    "    # If previews exist, try to melt all found files using the calendar-style logic\n",
    "    if previews:\n",
    "        def melt_calendar(path, year_hint=None):\n",
    "            xls = pd.ExcelFile(path)\n",
    "            # choose sheet containing aqi or first\n",
    "            sheet = next((s for s in xls.sheet_names if 'aqi' in s.lower()), xls.sheet_names[0])\n",
    "            df = pd.read_excel(path, sheet_name=sheet)\n",
    "            df.columns = [str(c).strip() for c in df.columns]\n",
    "            # find Day column tolerant\n",
    "            day_col = next((c for c in df.columns if 'day'==str(c).strip().lower() or str(c).strip().lower().startswith('day')), None)\n",
    "            if day_col is None:\n",
    "                # try numeric day-like header (1..31) or 'Date'\n",
    "                if any(str(c).strip().lower()=='date' for c in df.columns):\n",
    "                    # assume already long format\n",
    "                    df['Date'] = pd.to_datetime(df[[c for c in df.columns if 'date' in str(c).lower()][0]])\n",
    "                    if 'AQI' in df.columns:\n",
    "                        long = df[['Date','AQI']].copy()\n",
    "                        long['Year'] = long['Date'].dt.year\n",
    "                        long['Month'] = long['Date'].dt.month\n",
    "                        long['Day'] = long['Date'].dt.day\n",
    "                        return long[['Date','Year','Month','Day','AQI']]\n",
    "                    else:\n",
    "                        raise ValueError(\"Found Date column but no AQI column.\")\n",
    "                raise ValueError(\"No 'Day' column detected and no obvious 'Date' column.\")\n",
    "            # standard calendar melt\n",
    "            month_cols = [c for c in df.columns if c != day_col]\n",
    "            long = df.melt(id_vars=[day_col], value_vars=month_cols, var_name='MonthName', value_name='AQI')\n",
    "            long = long.dropna(subset=['AQI']).reset_index(drop=True)\n",
    "            long[day_col] = pd.to_numeric(long[day_col], errors='coerce').astype('Int64')\n",
    "            month_map = {calendar.month_name[i]: i for i in range(1,13)}\n",
    "            month_map.update({calendar.month_abbr[i]: i for i in range(1,13)})\n",
    "            long['Month'] = long['MonthName'].map(lambda x: month_map.get(str(x).strip(), np.nan)).astype('Int64')\n",
    "            # year from filename\n",
    "            if year_hint is None:\n",
    "                m = re.search(r\"(20\\d{2})\", os.path.basename(path))\n",
    "                year_hint = int(m.group(1)) if m else None\n",
    "            if year_hint is None:\n",
    "                raise ValueError(\"Cannot determine year for \" + path)\n",
    "            def make_date(r):\n",
    "                try:\n",
    "                    return pd.Timestamp(year=year_hint, month=int(r['Month']), day=int(r[day_col]))\n",
    "                except:\n",
    "                    return pd.NaT\n",
    "            long['Date'] = long.apply(make_date, axis=1)\n",
    "            long = long.dropna(subset=['Date']).reset_index(drop=True)\n",
    "            long['AQI'] = pd.to_numeric(long['AQI'], errors='coerce')\n",
    "            long['Year'] = year_hint\n",
    "            long = long.rename(columns={day_col:'Day'})\n",
    "            return long[['Date','Year','Month','Day','AQI']]\n",
    "\n",
    "        all_parts = []\n",
    "        for p in xlsx_paths:\n",
    "            try:\n",
    "                part = melt_calendar(p)\n",
    "                print(f\"\\nMELTED: {p} -> {len(part)} rows\")\n",
    "                all_parts.append(part)\n",
    "            except Exception as e:\n",
    "                print(f\"\\nSKIPPED {p} due to: {e}\")\n",
    "\n",
    "        if all_parts:\n",
    "            all_long = pd.concat(all_parts, ignore_index=True)\n",
    "            print(\"\\nCombined all_long rows:\", len(all_long))\n",
    "            print(all_long.head().to_string(index=False))\n",
    "        else:\n",
    "            print(\"\\nNo files could be melted automatically. Paste here one small sample of the sheet (first 8 rows and column names) and I will adapt the parser.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "223f5380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found files: []\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "No AQI files could be loaded. Check DATA_ROOT and file format.",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m No AQI files could be loaded. Check DATA_ROOT and file format.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sober\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:3587: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# CELL B: robustly load calendar-style Excel files and create all_long\n",
    "import pandas as pd, numpy as np, os, glob, calendar, re\n",
    "DATA_ROOT = r\"D:\\Downloads\\FieldProjectAQI_Data\"   # <-- ensure this matches CELL A\n",
    "pattern = os.path.join(DATA_ROOT, \"*.xlsx\")\n",
    "files = sorted(glob.glob(pattern))\n",
    "print(\"Found files:\", files)\n",
    "\n",
    "def melt_calendar(path, year_hint=None):\n",
    "    xls = pd.ExcelFile(path)\n",
    "    # choose first sheet or sheet named like 'AQI'\n",
    "    sheet = None\n",
    "    for s in xls.sheet_names:\n",
    "        if 'aqi' in s.lower():\n",
    "            sheet = s\n",
    "            break\n",
    "    if sheet is None:\n",
    "        sheet = xls.sheet_names[0]\n",
    "    df = pd.read_excel(path, sheet_name=sheet)\n",
    "    df.columns = [str(c).strip() for c in df.columns]\n",
    "    if 'Day' not in df.columns:\n",
    "        # try a more tolerant rename: find column name that looks like 'Day'\n",
    "        for c in df.columns:\n",
    "            if 'day' in c.lower():\n",
    "                df = df.rename(columns={c: 'Day'})\n",
    "                break\n",
    "    if 'Day' not in df.columns:\n",
    "        raise ValueError(f\"'Day' column missing in {path}; columns: {df.columns.tolist()}\")\n",
    "    month_cols = [c for c in df.columns if c != 'Day']\n",
    "    long = df.melt(id_vars=['Day'], value_vars=month_cols, var_name='MonthName', value_name='AQI')\n",
    "    long = long.dropna(subset=['AQI']).reset_index(drop=True)\n",
    "    # normalize Day to int\n",
    "    long['Day'] = pd.to_numeric(long['Day'], errors='coerce').astype('Int64')\n",
    "    # map month name to number\n",
    "    month_map = {calendar.month_name[i]: i for i in range(1,13)}\n",
    "    month_map.update({calendar.month_abbr[i]: i for i in range(1,13)})\n",
    "    long['Month'] = long['MonthName'].map(lambda x: month_map.get(str(x).strip(), np.nan)).astype('Int64')\n",
    "    # determine year: priority -> year_hint param -> filename -> try to find 4-digit year\n",
    "    if year_hint is None:\n",
    "        m = re.search(r\"(20\\d{2})\", os.path.basename(path))\n",
    "        year_hint = int(m.group(1)) if m else None\n",
    "    if year_hint is None:\n",
    "        raise ValueError(f\"Cannot determine year for file {path}. Provide year_hint or include year in filename.\")\n",
    "    def make_date(r):\n",
    "        try:\n",
    "            return pd.Timestamp(year=year_hint, month=int(r['Month']), day=int(r['Day']))\n",
    "        except Exception:\n",
    "            return pd.NaT\n",
    "    long['Date'] = long.apply(make_date, axis=1)\n",
    "    long = long.dropna(subset=['Date']).reset_index(drop=True)\n",
    "    long['AQI'] = pd.to_numeric(long['AQI'], errors='coerce')\n",
    "    long['Year'] = year_hint\n",
    "    # Keep only needed cols\n",
    "    return long[['Date','Year','Month','Day','AQI']]\n",
    "\n",
    "# Build combined DataFrame\n",
    "list_parts = []\n",
    "for f in files:\n",
    "    try:\n",
    "        part = melt_calendar(f)\n",
    "        print(f\"Loaded {os.path.basename(f)} -> rows {len(part)}\")\n",
    "        list_parts.append(part)\n",
    "    except Exception as e:\n",
    "        print(\"SKIP file\", f, \"error:\", e)\n",
    "\n",
    "if not list_parts:\n",
    "    raise SystemExit(\"No AQI files could be loaded. Check DATA_ROOT and file format.\")\n",
    "\n",
    "all_long = pd.concat(list_parts, ignore_index=True)\n",
    "print(\"Combined rows:\", len(all_long))\n",
    "print(all_long[['Date','Year','Month','Day','AQI']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff57d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AQI files found: []\n",
      "Prominent pollutant files found: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:17: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:17: SyntaxWarning: invalid escape sequence '\\D'\n",
      "C:\\Users\\sober\\AppData\\Local\\Temp\\ipykernel_17028\\3423529007.py:17: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  DATA_DIR = 'D:\\Downloads\\FieldProjectAQI_Data'\n"
     ]
    }
   ],
   "source": [
    "# AQI_and_Prominent_Plots_notebook.py\n",
    "# Ready-to-run notebook cells (paste into a .ipynb code cell) that:\n",
    "# - auto-detects your calendar-style AQI sheets for 2023/2024/2025\n",
    "# - melts Day x Month -> Date rows\n",
    "# - reads prominent-pollutant sheets (auto-detected by sheet name)\n",
    "# - produces and saves all requested PNG charts\n",
    "# Requirements: pandas, numpy, matplotlib\n",
    "\n",
    "# ---------------------- CELL 1: Imports & file discovery ----------------------\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import calendar\n",
    "\n",
    "DATA_DIR = 'D:\\Downloads\\FieldProjectAQI_Data'\n",
    "# Pattern-match files you uploaded; adjust if your filenames differ\n",
    "aqi_files = sorted(glob.glob(os.path.join(DATA_DIR, '*mumbai*202*.xlsx')))\n",
    "prom_files = sorted(glob.glob(os.path.join(DATA_DIR, 'Prominent*202*.xlsx')) +\n",
    "                    glob.glob(os.path.join(DATA_DIR, 'prominent*202*.xlsx')) +\n",
    "                    glob.glob(os.path.join(DATA_DIR, '*prominent*202*.xlsx')))\n",
    "\n",
    "print('AQI files found:', aqi_files)\n",
    "print('Prominent pollutant files found:', prom_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa616a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- CELL 2: Helpers to melt calendar-style AQI ----------------------\n",
    "\n",
    "def melt_calendar_aqi(path, year=None):\n",
    "    \"\"\"Read a calendar-style sheet (columns: Day, January..December) and return long DataFrame.\n",
    "    If the file contains multiple sheets, the function chooses the first sheet containing 'AQI' or uses sheet 0.\n",
    "    \"\"\"\n",
    "    xls = pd.ExcelFile(path)\n",
    "    # choose sheet containing 'AQI' (case-insensitive) else first sheet\n",
    "    sheet = None\n",
    "    for s in xls.sheet_names:\n",
    "        if 'aqi' in s.lower():\n",
    "            sheet = s\n",
    "            break\n",
    "    if sheet is None:\n",
    "        sheet = xls.sheet_names[0]\n",
    "    df = pd.read_excel(path, sheet_name=sheet)\n",
    "    df = df.rename(columns=lambda c: str(c).strip())\n",
    "    if 'Day' not in df.columns:\n",
    "        raise ValueError(f\"Expected column 'Day' in {path} (sheet: {sheet}). Columns: {df.columns.tolist()}\")\n",
    "    month_cols = [c for c in df.columns if str(c).strip().lower() != 'day']\n",
    "    long = df.melt(id_vars=['Day'], value_vars=month_cols, var_name='Month', value_name='AQI')\n",
    "    long = long.dropna(subset=['AQI'])\n",
    "    long['Day'] = pd.to_numeric(long['Day'], errors='coerce').astype('Int64')\n",
    "    month_map = {calendar.month_name[i]: i for i in range(1,13)}\n",
    "    month_map.update({calendar.month_abbr[i]: i for i in range(1,13)})\n",
    "    long['MonthNum'] = long['Month'].map(lambda x: month_map.get(str(x).strip(), np.nan))\n",
    "    # If year not provided, try to parse from filename\n",
    "    if year is None:\n",
    "        import re\n",
    "        m = re.search(r\"(20\\d{2})\", os.path.basename(path))\n",
    "        year = int(m.group(1)) if m else None\n",
    "    if year is None:\n",
    "        raise ValueError('Unable to determine year for ' + path)\n",
    "    def make_date(row):\n",
    "        try:\n",
    "            return pd.Timestamp(year=year, month=int(row['MonthNum']), day=int(row['Day']))\n",
    "        except Exception:\n",
    "            return pd.NaT\n",
    "    long['Date'] = long.apply(make_date, axis=1)\n",
    "    long = long.dropna(subset=['Date']).reset_index(drop=True)\n",
    "    long['AQI'] = pd.to_numeric(long['AQI'], errors='coerce')\n",
    "    long['Year'] = year\n",
    "    long['Month'] = long['MonthNum']\n",
    "    return long[['Date','Year','Month','Day','AQI']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c63eb1",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "No AQI files found in D:\\Downloads\\FieldProjectAQI_Data. Please check the folder and filename pattern.",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m No AQI files found in D:\\Downloads\\FieldProjectAQI_Data. Please check the folder and filename pattern.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sober\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:3587: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# ---------------------- CELL 3: Load all AQI files into a single DataFrame ----------------------\n",
    "all_long = []\n",
    "if not aqi_files:\n",
    "    raise SystemExit(f'No AQI files found in {DATA_DIR}. Please check the folder and filename pattern.')\n",
    "\n",
    "for p in aqi_files:\n",
    "    try:\n",
    "        df_long = melt_calendar_aqi(p)\n",
    "        all_long.append(df_long)\n",
    "    except Exception as e:\n",
    "        print('Failed to melt', p, '->', str(e))\n",
    "\n",
    "if not all_long:\n",
    "    raise SystemExit(f'No AQI long-form data was created. Check file formats and file names in {DATA_DIR}.')\n",
    "\n",
    "all_long = pd.concat(all_long, ignore_index=True)\n",
    "all_long['Month'] = all_long['Month'].astype(int)\n",
    "print('Combined rows:', len(all_long))\n",
    "\n",
    "# Optional quick preview (uncomment if running interactively)\n",
    "# display(all_long.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073eb1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No prominent pollutant data parsed automatically.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------- CELL 4: Read prominent pollutant sheets (auto-detect) ----------------------\n",
    "# This function looks for a sheet whose name contains 'prominent' or 'dominant' or 'pollutant'\n",
    "\n",
    "def read_prominent_file(path):\n",
    "    xls = pd.ExcelFile(path)\n",
    "    candidate = None\n",
    "    for s in xls.sheet_names:\n",
    "        lname = s.lower()\n",
    "        if any(k in lname for k in ['prominent','dominant','pollutant','param']):\n",
    "            candidate = s\n",
    "            break\n",
    "    if candidate is None:\n",
    "        candidate = xls.sheet_names[0]\n",
    "    df = pd.read_excel(path, sheet_name=candidate)\n",
    "    df = df.rename(columns=lambda c: str(c).strip())\n",
    "    return df, candidate\n",
    "\n",
    "prom_long_list = []\n",
    "for p in prom_files:\n",
    "    try:\n",
    "        dfp, sheet = read_prominent_file(p)\n",
    "        print('Reading', p, 'sheet:', sheet, 'columns:', list(dfp.columns)[:10])\n",
    "        # Attempt to melt similar calendar-style if it has Day + months\n",
    "        if 'Day' in dfp.columns:\n",
    "            months = [c for c in dfp.columns if str(c).strip().lower()!='day']\n",
    "            tmp = dfp.melt(id_vars=['Day'], value_vars=months, var_name='Month', value_name='Pollutant')\n",
    "            tmp['Day'] = pd.to_numeric(tmp['Day'], errors='coerce').astype('Int64')\n",
    "            month_map = {calendar.month_name[i]: i for i in range(1,13)}\n",
    "            month_map.update({calendar.month_abbr[i]: i for i in range(1,13)})\n",
    "            tmp['MonthNum'] = tmp['Month'].map(lambda x: month_map.get(str(x).strip(), np.nan))\n",
    "            # get year from filename\n",
    "            import re\n",
    "            m = re.search(r\"(20\\d{2})\", os.path.basename(p))\n",
    "            year = int(m.group(1)) if m else None\n",
    "            def make_date(row):\n",
    "                try:\n",
    "                    return pd.Timestamp(year=year, month=int(row['MonthNum']), day=int(row['Day']))\n",
    "                except:\n",
    "                    return pd.NaT\n",
    "            tmp['Date'] = tmp.apply(make_date, axis=1)\n",
    "            tmp = tmp.dropna(subset=['Date']).reset_index(drop=True)\n",
    "            tmp['Year'] = year\n",
    "            tmp = tmp[['Date','Year','MonthNum','Day','Pollutant']].rename(columns={'MonthNum':'Month'})\n",
    "            prom_long_list.append(tmp)\n",
    "        else:\n",
    "            print('Prominent sheet not in Day x Month layout; trying to find Date or Day columns.')\n",
    "            # If it already has a Date column and a Pollutant column\n",
    "            possible_date = None\n",
    "            for col in dfp.columns:\n",
    "                if 'date' in col.lower():\n",
    "                    possible_date = col\n",
    "                    break\n",
    "            poll_col = None\n",
    "            for col in dfp.columns:\n",
    "                if any(k in col.lower() for k in ['dominant','prominent','pollutant','param','characteristic']):\n",
    "                    poll_col = col\n",
    "                    break\n",
    "            if possible_date is not None and poll_col is not None:\n",
    "                tmp = dfp[[possible_date, poll_col]].dropna()\n",
    "                tmp.columns = ['Date','Pollutant']\n",
    "                tmp['Date'] = pd.to_datetime(tmp['Date'], errors='coerce')\n",
    "                tmp = tmp.dropna(subset=['Date']).reset_index(drop=True)\n",
    "                tmp['Year'] = tmp['Date'].dt.year\n",
    "                tmp['Month'] = tmp['Date'].dt.month\n",
    "                prom_long_list.append(tmp[['Date','Year','Month','Pollutant']])\n",
    "            else:\n",
    "                print('Could not auto-parse prominent pollutant sheet', p, '- please inspect the sheet manually if parsing fails.')\n",
    "    except Exception as e:\n",
    "        print('Error reading prominent file', p, ':', e)\n",
    "\n",
    "if prom_long_list:\n",
    "    prom_all = pd.concat(prom_long_list, ignore_index=True)\n",
    "    prom_all['Pollutant'] = prom_all['Pollutant'].astype(str).str.strip()\n",
    "    print('Prominent pollutant records:', len(prom_all))\n",
    "else:\n",
    "    prom_all = pd.DataFrame()\n",
    "    print('No prominent pollutant data parsed automatically.')#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b8292b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Year'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(OUTPUT_DIR, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# 3-Year Comparative Monthly Averages\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m monthly_avg \u001b[38;5;241m=\u001b[39m \u001b[43mall_long\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mYear\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMonth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAQI\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[0;32m     21\u001b[0m pivot_monthly \u001b[38;5;241m=\u001b[39m monthly_avg\u001b[38;5;241m.\u001b[39mpivot(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMonth\u001b[39m\u001b[38;5;124m'\u001b[39m, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m'\u001b[39m, values\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAQI\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreindex(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m13\u001b[39m))\n\u001b[0;32m     22\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m14\u001b[39m, \u001b[38;5;241m7\u001b[39m)) \u001b[38;5;66;03m# Increased figure size for better spacing\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\frame.py:9183\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[1;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[0;32m   9180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   9181\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 9183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   9184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9186\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9189\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9193\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\groupby\\groupby.py:1329\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[1;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna \u001b[38;5;241m=\u001b[39m dropna\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1329\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1334\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_default\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1337\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[0;32m   1340\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ping\u001b[38;5;241m.\u001b[39m_passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouper\u001b[38;5;241m.\u001b[39mgroupings):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\groupby\\grouper.py:1043\u001b[0m, in \u001b[0;36mget_grouper\u001b[1;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[0;32m   1041\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1043\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[0;32m   1044\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1045\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Year'"
     ]
    }
   ],
   "source": [
    "# CELL 5 (REVISED): Plotting functions & CPCB categories\n",
    "# ==============================================================================\n",
    "\n",
    "def aqi_category(a):\n",
    "    try:\n",
    "        a = float(a)\n",
    "    except (ValueError, TypeError):\n",
    "        return np.nan\n",
    "    if a <= 50: return 'Good'\n",
    "    if a <= 100: return 'Satisfactory'\n",
    "    if a <= 200: return 'Moderate'\n",
    "    if a <= 300: return 'Poor'\n",
    "    if a <= 400: return 'Very Poor'\n",
    "    return 'Severe'\n",
    "\n",
    "OUTPUT_DIR = DATA_DIR\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# 3-Year Comparative Monthly Averages\n",
    "monthly_avg = all_long.groupby(['Year','Month'])['AQI'].mean().reset_index()\n",
    "pivot_monthly = monthly_avg.pivot(index='Month', columns='Year', values='AQI').reindex(index=range(1,13))\n",
    "plt.figure(figsize=(14, 7)) # Increased figure size for better spacing\n",
    "pivot_monthly.plot(kind='bar', width=0.8, ax=plt.gca()) # Use ax=plt.gca() to plot on the current figure\n",
    "plt.xlabel('Month', fontsize=12)\n",
    "plt.ylabel('Average AQI', fontsize=12)\n",
    "plt.title('3-Year Comparative Monthly Average AQI (Mumbai)', fontsize=16)\n",
    "plt.xticks(ticks=range(12), labels=[calendar.month_abbr[i+1] for i in range(12)], rotation=0)\n",
    "# FIX: Move legend to a better position\n",
    "plt.legend(title='Year', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout(rect=[0, 0, 0.9, 1]) # Adjust layout to make space for legend\n",
    "out1 = os.path.join(OUTPUT_DIR, '3yr_monthly_avg_aqi.png')\n",
    "plt.savefig(out1)\n",
    "plt.close()\n",
    "print('Saved', out1)\n",
    "\n",
    "# Overall Annual AQI Trend (bar chart for better label visibility)\n",
    "annual_avg = all_long.groupby('Year')['AQI'].mean().reset_index()\n",
    "plt.figure(figsize=(7, 5))\n",
    "# FIX: Switched to a bar chart and adjusted x-ticks for clarity\n",
    "bars = plt.bar(annual_avg['Year'], annual_avg['AQI'], color=['skyblue', 'salmon', 'lightgreen'])\n",
    "plt.xlabel('Year', fontsize=12)\n",
    "plt.ylabel('Average AQI', fontsize=12)\n",
    "plt.title('Overall Annual Average AQI', fontsize=14)\n",
    "# FIX: Ensure x-axis ticks are integers for the years\n",
    "plt.xticks(annual_avg['Year'].astype(int))\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "out2 = os.path.join(OUTPUT_DIR, 'annual_avg_aqi.png')\n",
    "plt.savefig(out2)\n",
    "plt.close()\n",
    "print('Saved', out2)\n",
    "\n",
    "# ... (The code for Daily Trends and Heatmaps remains the same as it didn't have issues) ...\n",
    "# Daily AQI Trends for Each Year\n",
    "for y in sorted(all_long['Year'].unique()):\n",
    "    dfy = all_long[all_long['Year']==y].sort_values('Date')\n",
    "    plt.figure(figsize=(14,4))\n",
    "    plt.plot(dfy['Date'], dfy['AQI'])\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('AQI')\n",
    "    plt.title(f'Daily AQI Trend - {y}')\n",
    "    plt.tight_layout()\n",
    "    out = os.path.join(OUTPUT_DIR, f'daily_trend_{y}.png')\n",
    "    plt.savefig(out)\n",
    "    plt.close()\n",
    "    print('Saved', out)\n",
    "\n",
    "# AQI Heatmap (Month vs Day) for each year\n",
    "for y in sorted(all_long['Year'].unique()):\n",
    "    dfy = all_long[all_long['Year']==y].copy()\n",
    "    heat = dfy.pivot_table(index='Month', columns='Day', values='AQI', aggfunc='mean').reindex(index=range(1,13))\n",
    "    plt.figure(figsize=(14,6))\n",
    "    plt.imshow(heat, aspect='auto', origin='lower')\n",
    "    plt.colorbar(label='AQI')\n",
    "    plt.yticks(ticks=np.arange(12), labels=[calendar.month_name[i+1] for i in range(12)])\n",
    "    plt.xticks(ticks=np.arange(31), labels=list(range(1,32)))\n",
    "    plt.xlabel('Day of Month')\n",
    "    plt.ylabel('Month')\n",
    "    plt.title(f'AQI Heatmap (Month vs Day) - {y}')\n",
    "    plt.tight_layout()\n",
    "    out = os.path.join(OUTPUT_DIR, f'aqi_heatmap_{y}.png')\n",
    "    plt.savefig(out)\n",
    "    plt.close()\n",
    "    print('Saved', out)\n",
    "\n",
    "# AQI Category Distribution by Year\n",
    "all_long['Category'] = all_long['AQI'].apply(aqi_category)\n",
    "cat_counts = all_long.groupby(['Year','Category']).size().reset_index(name='days')\n",
    "pivot_cat = cat_counts.pivot(index='Category', columns='Year', values='days').reindex(index=['Good','Satisfactory','Moderate','Poor','Very Poor','Severe']).fillna(0)\n",
    "plt.figure(figsize=(10, 6)) # Increased figure size\n",
    "pivot_cat.plot(kind='bar', ax=plt.gca())\n",
    "plt.xlabel('AQI Category', fontsize=12)\n",
    "plt.ylabel('Days', fontsize=12)\n",
    "plt.title('AQI Category Distribution by Year', fontsize=16)\n",
    "# FIX: Rotate labels for better visibility\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "# FIX: Move legend to a better position\n",
    "plt.legend(title='Year', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout(rect=[0, 0, 0.85, 1]) # Adjust layout for legend\n",
    "out = os.path.join(OUTPUT_DIR, 'aqi_category_distribution.png')\n",
    "plt.savefig(out)\n",
    "plt.close()\n",
    "print('Saved', out)\n",
    "\n",
    "# Monthly AQI Distribution Boxplot\n",
    "monthly_boxes = [all_long[all_long['Month']==m]['AQI'].dropna().values for m in range(1,13)]\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.boxplot(monthly_boxes, labels=[calendar.month_abbr[m] for m in range(1,13)])\n",
    "plt.xlabel('Month', fontsize=12)\n",
    "plt.ylabel('AQI', fontsize=12)\n",
    "plt.title('Monthly AQI Distribution (Boxplot) - All years combined', fontsize=16)\n",
    "plt.tight_layout()\n",
    "out = os.path.join(OUTPUT_DIR, 'monthly_aqi_boxplot.png')\n",
    "plt.savefig(out)\n",
    "plt.close()\n",
    "print('Saved', out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7899dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prominent pollutant dataset is empty â€” pollutant-specific charts were skipped.\n",
      "\n",
      "All done. Check the PNG files in D:\\Downloads\\FieldProjectAQI_Data\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CELL 6 (REVISED): Pollutant-specific charts (if data exists)\n",
    "# ==============================================================================\n",
    "if not prom_all.empty:\n",
    "    prom_all['Year'] = prom_all['Date'].dt.year\n",
    "    prom_all['Month'] = prom_all['Date'].dt.month\n",
    "    monthly_prom = prom_all.groupby(['Year','Month','Pollutant']).size().reset_index(name='days')\n",
    "    for y in sorted(prom_all['Year'].unique()):\n",
    "        dfy = monthly_prom[monthly_prom['Year']==y]\n",
    "        pivot = dfy.pivot(index='Month', columns='Pollutant', values='days').reindex(index=range(1,13)).fillna(0)\n",
    "        plt.figure(figsize=(12, 6)) # Increased figure size\n",
    "        pivot.plot(kind='bar', stacked=True, ax=plt.gca())\n",
    "        plt.xlabel('Month', fontsize=12)\n",
    "        plt.ylabel('Days (count)', fontsize=12)\n",
    "        plt.title(f'Monthly Prominent Pollutant Breakdown - {y}', fontsize=16)\n",
    "        plt.xticks(ticks=range(12), labels=[calendar.month_abbr[i+1] for i in range(12)], rotation=0)\n",
    "        # FIX: Move legend to a better position\n",
    "        plt.legend(title='Pollutant', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout(rect=[0, 0, 0.8, 1]) # Adjust layout for legend\n",
    "        out = os.path.join(OUTPUT_DIR, f'monthly_prominent_pollutant_{y}.png')\n",
    "        plt.savefig(out)\n",
    "        plt.close()\n",
    "        print('Saved', out)\n",
    "\n",
    "    # Overall pollutant dominance (pie)\n",
    "    overall = prom_all['Pollutant'].value_counts()\n",
    "    plt.figure(figsize=(10, 8)) # Increased figure size\n",
    "    # FIX: Create a function to prevent label overlap on the pie chart\n",
    "    def autopct_generator(limit):\n",
    "        \"\"\"Don't show percentage label for slices smaller than limit.\"\"\"\n",
    "        def inner_autopct(pct):\n",
    "            return ('%1.1f%%' % pct) if pct > limit else ''\n",
    "        return inner_autopct\n",
    "    \n",
    "    overall.plot(kind='pie', autopct=autopct_generator(3), ylabel='', textprops={'fontsize': 10})\n",
    "    plt.title('Overall Pollutant Dominance (All years)', fontsize=16)\n",
    "    # FIX: Create a legend for the pie chart instead of labels on the slices\n",
    "    plt.legend(labels=overall.index, bbox_to_anchor=(1.15, 0.9), loc=\"upper right\", title=\"Pollutants\")\n",
    "    plt.tight_layout()\n",
    "    out = os.path.join(OUTPUT_DIR, 'overall_pollutant_dominance.png')\n",
    "    plt.savefig(out)\n",
    "    plt.close()\n",
    "    print('Saved', out)\n",
    "else:\n",
    "    print('Prominent pollutant dataset is empty â€” pollutant-specific charts were skipped.')\n",
    "\n",
    "print('\\nAll done. Check the PNG files in', OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0538cb5",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'FieldProjectAQI_Data\\data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# after running the notebook that produced all_long and prom_all\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mall_long\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFieldProjectAQI_Data/data/cleaned_aqi.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m prom_all\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m      4\u001b[0m     prom_all\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFieldProjectAQI_Data/data/prominent_pollutant.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\generic.py:3967\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3956\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3958\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3959\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3960\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3964\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3965\u001b[0m )\n\u001b[1;32m-> 3967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3970\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3972\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3984\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\formats\\format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1013\u001b[0m )\n\u001b[1;32m-> 1014\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\formats\\csvs.py:251\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    268\u001b[0m     )\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\common.py:749\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[1;32m--> 749\u001b[0m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[0;32m    752\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzstd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\common.py:616\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    614\u001b[0m parent \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mparent\n\u001b[0;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[1;32m--> 616\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot save file into a non-existent directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mOSError\u001b[0m: Cannot save file into a non-existent directory: 'FieldProjectAQI_Data\\data'"
     ]
    }
   ],
   "source": [
    "# after running the notebook that produced all_long and prom_all\n",
    "all_long.to_csv(r'FieldProjectAQI_Data/data/cleaned_aqi.csv', index=False)\n",
    "if not prom_all.empty:\n",
    "    prom_all.to_csv(r'FieldProjectAQI_Data/data/prominent_pollutant.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
